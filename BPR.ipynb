{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CkfLOvwT_R8",
        "outputId": "b89ec261-a934-4f76-81ab-fc1672c9f5c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total interactions (after dropping NaNs): 239375 (Dropped 1 rows)\n"
          ]
        }
      ],
      "source": [
        "# Tahap 1: Load data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Baca file CSV\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Hapus baris dengan nilai NaN di 'item_id'\n",
        "initial_len = len(train)\n",
        "train.dropna(subset=['item_id'], inplace=True)\n",
        "print(f\"Total interactions (after dropping NaNs): {len(train)} (Dropped {initial_len - len(train)} rows)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy25ywhqXBqp",
        "outputId": "806eb34f-d5a5-4a6d-ef1a-f36f17d7bb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 187052, Test: 52323\n"
          ]
        }
      ],
      "source": [
        "# Tahap 2: Split Data jadi train-test per user\n",
        "train_list, test_list = [], []\n",
        "\n",
        "for uid, group in train.groupby(\"user_id\"):\n",
        "    if len(group) < 2:\n",
        "        train_list.append(group)\n",
        "        continue\n",
        "    tr, te = train_test_split(group, test_size=0.2, random_state=42)\n",
        "    train_list.append(tr)\n",
        "    test_list.append(te)\n",
        "\n",
        "train_split = pd.concat(train_list)\n",
        "test_split = pd.concat(test_list)\n",
        "print(f\"Train: {len(train_split)}, Test: {len(test_split)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YIjrxgSzJII",
        "outputId": "75df15b1-4bc4-416b-8bda-6a6876703d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Users: 12160, Items: 95211\n"
          ]
        }
      ],
      "source": [
        "# Tahap 3: Encode user & item ke index numerik\n",
        "user_to_index = {u: i for i, u in enumerate(train_split['user_id'].unique())}\n",
        "item_to_index = {i: j for j, i in enumerate(train_split['item_id'].unique())}\n",
        "index_to_user = {i: u for u, i in user_to_index.items()}\n",
        "index_to_item = {j: i for i, j in item_to_index.items()}\n",
        "\n",
        "user_index = train_split['user_id'].map(user_to_index)\n",
        "item_index = train_split['item_id'].map(item_to_index)\n",
        "\n",
        "n_users = len(user_to_index)\n",
        "n_items = len(item_to_index)\n",
        "print(f\"Users: {n_users}, Items: {n_items}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEvDBBrdXC9t",
        "outputId": "d2d706d3-dfbd-488b-c929-9c7b0f88a18c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSR matrix shape: (12160, 95211)\n"
          ]
        }
      ],
      "source": [
        "# Tahap 4: Buat weighted user-item matrix (TF-IDF style)\n",
        "item_freq = train_split['item_id'].value_counts()\n",
        "item_weight = 1.0 / np.log1p(item_freq)  # Semakin populer, bobot makin kecil\n",
        "train_split['weight'] = train_split['item_id'].map(item_weight)\n",
        "\n",
        "data = train_split['weight'].values.astype(np.float32)\n",
        "\n",
        "user_item_csr = csr_matrix((data, (user_index, item_index)), shape=(n_users, n_items))\n",
        "print(\"CSR matrix shape:\", user_item_csr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "gNdXHfGtPxI1"
      },
      "outputs": [],
      "source": [
        "# Tahap 5: Persiapan Data untuk BPR\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Membentuk pasangan positif (user_idx, item_idx)\n",
        "pos_pairs = list(\n",
        "    zip(train_split['user_id'].map(user_to_index),\n",
        "        train_split['item_id'].map(item_to_index))\n",
        ")\n",
        "\n",
        "# Set item yang pernah di-like per user\n",
        "user_pos = defaultdict(set)\n",
        "for u, i in pos_pairs:\n",
        "    user_pos[u].add(i)\n",
        "\n",
        "# Hitung frekuensi item untuk popularity negative sampling\n",
        "item_freq_series = train_split['item_id'].map(item_to_index).value_counts()\n",
        "item_freq = np.zeros(n_items, dtype=np.float32)\n",
        "for idx, cnt in item_freq_series.items():\n",
        "    item_freq[idx] = cnt\n",
        "\n",
        "# Distribusi peluang sampling (power 0.75)\n",
        "power = 0.75\n",
        "freq = item_freq / item_freq.sum()\n",
        "item_prob_np = freq ** 0.75\n",
        "item_prob_np = item_prob_np / item_prob_np.sum()\n",
        "popular_item_indices = np.arange(n_items)\n",
        "pos_pairs_array = np.array(pos_pairs, dtype=np.int32)\n",
        "n_pos = len(pos_pairs_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "i_FDOzP7QAkg"
      },
      "outputs": [],
      "source": [
        "# Tahap 6: Defenisi Model BPR\n",
        "class BPRModel(nn.Module):\n",
        "    def __init__(self, n_users, n_items, n_factors=128):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
        "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
        "\n",
        "        nn.init.normal_(self.user_emb.weight, std=0.01)\n",
        "        nn.init.normal_(self.item_emb.weight, std=0.01)\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        u_vec = self.user_emb(u)\n",
        "        i_vec = self.item_emb(i)\n",
        "        j_vec = self.item_emb(j)\n",
        "        x_ui = torch.sum(u_vec * i_vec, dim=1)\n",
        "        x_uj = torch.sum(u_vec * j_vec, dim=1)\n",
        "        return x_ui, x_uj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "wC4y2CC6QE9d"
      },
      "outputs": [],
      "source": [
        "# Tahap 7:Training BPR\n",
        "factors = 256\n",
        "lr = 0.001\n",
        "warmup_epochs = 5\n",
        "batch_size = 4096\n",
        "neg_samples = 2\n",
        "epochs = 200\n",
        "reg = 1e-4\n",
        "\n",
        "model = BPRModel(n_users, n_items, n_factors=factors).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "def sample_batch(batch_size):\n",
        "    idx = np.random.randint(0, n_pos, size=batch_size)\n",
        "    batch = pos_pairs_array[idx]\n",
        "\n",
        "    users = batch[:, 0]\n",
        "    pos_items = batch[:, 1]\n",
        "\n",
        "    neg_items = np.random.choice(\n",
        "        popular_item_indices,\n",
        "        size=batch_size * neg_samples,\n",
        "        p=item_prob_np\n",
        "    ).reshape(batch_size, neg_samples)\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        u = users[b]\n",
        "        for k in range(neg_samples):\n",
        "            neg = neg_items[b, k]\n",
        "            attempt = 0\n",
        "            while neg in user_pos[u] and attempt < 10:\n",
        "                neg = np.random.choice(popular_item_indices, p=item_prob_np)\n",
        "                attempt += 1\n",
        "            neg_items[b, k] = neg\n",
        "\n",
        "    return users.astype(np.int64), pos_items.astype(np.int64), neg_items[:, 0].astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o81nHKQjhuu",
        "outputId": "b5dbd7d1-03dc-4c2c-9894-c2bf10269fad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mulai training BPR...\n",
            "\n",
            "Epoch 1/200 â€” Loss: 31.1782\n",
            "Epoch 2/200 â€” Loss: 31.1231\n",
            "Epoch 3/200 â€” Loss: 30.9959\n",
            "Epoch 4/200 â€” Loss: 30.7270\n",
            "Epoch 5/200 â€” Loss: 30.4555\n",
            "Epoch 6/200 â€” Loss: 30.3687\n",
            "Epoch 7/200 â€” Loss: 30.0013\n",
            "Epoch 8/200 â€” Loss: 28.8662\n",
            "Epoch 9/200 â€” Loss: 26.4299\n",
            "Epoch 10/200 â€” Loss: 23.0468\n",
            "Epoch 11/200 â€” Loss: 19.8981\n",
            "Epoch 12/200 â€” Loss: 17.9608\n",
            "Epoch 13/200 â€” Loss: 17.2047\n",
            "Epoch 14/200 â€” Loss: 17.0878\n",
            "Epoch 15/200 â€” Loss: 16.8902\n",
            "Epoch 16/200 â€” Loss: 16.0490\n",
            "Epoch 17/200 â€” Loss: 14.3193\n",
            "Epoch 18/200 â€” Loss: 12.2486\n",
            "Epoch 19/200 â€” Loss: 10.3091\n",
            "Epoch 20/200 â€” Loss: 8.9200\n",
            "Epoch 21/200 â€” Loss: 8.1583\n",
            "Epoch 22/200 â€” Loss: 7.9076\n",
            "Epoch 23/200 â€” Loss: 7.8626\n",
            "Epoch 24/200 â€” Loss: 7.7297\n",
            "Epoch 25/200 â€” Loss: 7.2763\n",
            "Epoch 26/200 â€” Loss: 6.4751\n",
            "Epoch 27/200 â€” Loss: 5.5723\n",
            "Epoch 28/200 â€” Loss: 4.7650\n",
            "Epoch 29/200 â€” Loss: 4.2128\n",
            "Epoch 30/200 â€” Loss: 3.9305\n",
            "Epoch 31/200 â€” Loss: 3.8267\n",
            "Epoch 32/200 â€” Loss: 3.8160\n",
            "Epoch 33/200 â€” Loss: 3.7523\n",
            "Epoch 34/200 â€” Loss: 3.5532\n",
            "Epoch 35/200 â€” Loss: 3.1929\n",
            "Epoch 36/200 â€” Loss: 2.7926\n",
            "Epoch 37/200 â€” Loss: 2.4738\n",
            "Epoch 38/200 â€” Loss: 2.2477\n",
            "Epoch 39/200 â€” Loss: 2.1221\n",
            "Epoch 40/200 â€” Loss: 2.0934\n",
            "Epoch 41/200 â€” Loss: 2.0822\n",
            "Epoch 42/200 â€” Loss: 2.0501\n",
            "Epoch 43/200 â€” Loss: 1.9563\n",
            "Epoch 44/200 â€” Loss: 1.7906\n",
            "Epoch 45/200 â€” Loss: 1.6078\n",
            "Epoch 46/200 â€” Loss: 1.4485\n",
            "Epoch 47/200 â€” Loss: 1.3491\n",
            "Epoch 48/200 â€” Loss: 1.3077\n",
            "Epoch 49/200 â€” Loss: 1.2883\n",
            "Epoch 50/200 â€” Loss: 1.2873\n",
            "Epoch 51/200 â€” Loss: 1.2608\n",
            "Epoch 52/200 â€” Loss: 1.2187\n",
            "Epoch 53/200 â€” Loss: 1.1216\n",
            "Epoch 54/200 â€” Loss: 1.0331\n",
            "Epoch 55/200 â€” Loss: 0.9600\n",
            "Epoch 56/200 â€” Loss: 0.9086\n",
            "Epoch 57/200 â€” Loss: 0.8807\n",
            "Epoch 58/200 â€” Loss: 0.8785\n",
            "Epoch 59/200 â€” Loss: 0.8754\n",
            "Epoch 60/200 â€” Loss: 0.8596\n",
            "Epoch 61/200 â€” Loss: 0.8305\n",
            "Epoch 62/200 â€” Loss: 0.7828\n",
            "Epoch 63/200 â€” Loss: 0.7300\n",
            "Epoch 64/200 â€” Loss: 0.6881\n",
            "Epoch 65/200 â€” Loss: 0.6601\n",
            "Epoch 66/200 â€” Loss: 0.6465\n",
            "Epoch 67/200 â€” Loss: 0.6454\n",
            "Epoch 68/200 â€” Loss: 0.6431\n",
            "Epoch 69/200 â€” Loss: 0.6322\n",
            "Epoch 70/200 â€” Loss: 0.6196\n",
            "Epoch 71/200 â€” Loss: 0.5816\n",
            "Epoch 72/200 â€” Loss: 0.5531\n",
            "Epoch 73/200 â€” Loss: 0.5301\n",
            "Epoch 74/200 â€” Loss: 0.5162\n",
            "Epoch 75/200 â€” Loss: 0.5043\n",
            "Epoch 76/200 â€” Loss: 0.5060\n",
            "Epoch 77/200 â€” Loss: 0.5044\n",
            "Epoch 78/200 â€” Loss: 0.4971\n",
            "Epoch 79/200 â€” Loss: 0.4820\n",
            "Epoch 80/200 â€” Loss: 0.4675\n",
            "Epoch 81/200 â€” Loss: 0.4478\n",
            "Epoch 82/200 â€” Loss: 0.4327\n",
            "Epoch 83/200 â€” Loss: 0.4203\n",
            "Epoch 84/200 â€” Loss: 0.4157\n",
            "Epoch 85/200 â€” Loss: 0.4176\n",
            "Epoch 86/200 â€” Loss: 0.4174\n",
            "Epoch 87/200 â€” Loss: 0.4133\n",
            "Epoch 88/200 â€” Loss: 0.4029\n",
            "Epoch 89/200 â€” Loss: 0.3890\n",
            "Epoch 90/200 â€” Loss: 0.3804\n",
            "Epoch 91/200 â€” Loss: 0.3660\n",
            "Epoch 92/200 â€” Loss: 0.3600\n",
            "Epoch 93/200 â€” Loss: 0.3586\n",
            "Epoch 94/200 â€” Loss: 0.3584\n",
            "Epoch 95/200 â€” Loss: 0.3558\n",
            "Epoch 96/200 â€” Loss: 0.3544\n",
            "Epoch 97/200 â€” Loss: 0.3462\n",
            "Epoch 98/200 â€” Loss: 0.3404\n",
            "Epoch 99/200 â€” Loss: 0.3289\n",
            "Epoch 100/200 â€” Loss: 0.3231\n",
            "Epoch 101/200 â€” Loss: 0.3195\n",
            "Epoch 102/200 â€” Loss: 0.3178\n",
            "Epoch 103/200 â€” Loss: 0.3173\n",
            "Epoch 104/200 â€” Loss: 0.3152\n",
            "Epoch 105/200 â€” Loss: 0.3132\n",
            "Epoch 106/200 â€” Loss: 0.3093\n",
            "Epoch 107/200 â€” Loss: 0.3011\n",
            "Epoch 108/200 â€” Loss: 0.2975\n",
            "Epoch 109/200 â€” Loss: 0.2903\n",
            "Epoch 110/200 â€” Loss: 0.2888\n",
            "Epoch 111/200 â€” Loss: 0.2883\n",
            "Epoch 112/200 â€” Loss: 0.2877\n",
            "Epoch 113/200 â€” Loss: 0.2879\n",
            "Epoch 114/200 â€” Loss: 0.2857\n",
            "Epoch 115/200 â€” Loss: 0.2824\n",
            "Epoch 116/200 â€” Loss: 0.2765\n",
            "Epoch 117/200 â€” Loss: 0.2731\n",
            "Epoch 118/200 â€” Loss: 0.2703\n",
            "Epoch 119/200 â€” Loss: 0.2681\n",
            "Epoch 120/200 â€” Loss: 0.2667\n",
            "Epoch 121/200 â€” Loss: 0.2658\n",
            "Epoch 122/200 â€” Loss: 0.2657\n",
            "Epoch 123/200 â€” Loss: 0.2640\n",
            "Epoch 124/200 â€” Loss: 0.2618\n",
            "Epoch 125/200 â€” Loss: 0.2575\n",
            "Epoch 126/200 â€” Loss: 0.2562\n",
            "Epoch 127/200 â€” Loss: 0.2530\n",
            "Epoch 128/200 â€” Loss: 0.2523\n",
            "Epoch 129/200 â€” Loss: 0.2517\n",
            "Epoch 130/200 â€” Loss: 0.2508\n",
            "Epoch 131/200 â€” Loss: 0.2507\n",
            "Epoch 132/200 â€” Loss: 0.2501\n",
            "Epoch 133/200 â€” Loss: 0.2480\n",
            "Epoch 134/200 â€” Loss: 0.2441\n",
            "Epoch 135/200 â€” Loss: 0.2433\n",
            "Epoch 136/200 â€” Loss: 0.2412\n",
            "Epoch 137/200 â€” Loss: 0.2397\n",
            "Epoch 138/200 â€” Loss: 0.2401\n",
            "Epoch 139/200 â€” Loss: 0.2383\n",
            "Epoch 140/200 â€” Loss: 0.2388\n",
            "Epoch 141/200 â€” Loss: 0.2372\n",
            "Epoch 142/200 â€” Loss: 0.2370\n",
            "Epoch 143/200 â€” Loss: 0.2340\n",
            "Epoch 144/200 â€” Loss: 0.2317\n",
            "Epoch 145/200 â€” Loss: 0.2317\n",
            "Epoch 146/200 â€” Loss: 0.2302\n",
            "Epoch 147/200 â€” Loss: 0.2312\n",
            "Epoch 148/200 â€” Loss: 0.2294\n",
            "Epoch 149/200 â€” Loss: 0.2300\n",
            "Epoch 150/200 â€” Loss: 0.2303\n",
            "Epoch 151/200 â€” Loss: 0.2272\n",
            "Epoch 152/200 â€” Loss: 0.2266\n",
            "Epoch 153/200 â€” Loss: 0.2247\n",
            "Epoch 154/200 â€” Loss: 0.2237\n",
            "Epoch 155/200 â€” Loss: 0.2230\n",
            "Epoch 156/200 â€” Loss: 0.2231\n",
            "Epoch 157/200 â€” Loss: 0.2227\n",
            "Epoch 158/200 â€” Loss: 0.2237\n",
            "Epoch 159/200 â€” Loss: 0.2219\n",
            "Epoch 160/200 â€” Loss: 0.2210\n",
            "Epoch 161/200 â€” Loss: 0.2194\n",
            "Epoch 162/200 â€” Loss: 0.2178\n",
            "Epoch 163/200 â€” Loss: 0.2176\n",
            "Epoch 164/200 â€” Loss: 0.2173\n",
            "Epoch 165/200 â€” Loss: 0.2162\n",
            "Epoch 166/200 â€” Loss: 0.2169\n",
            "Epoch 167/200 â€” Loss: 0.2165\n",
            "Epoch 168/200 â€” Loss: 0.2160\n",
            "Epoch 169/200 â€” Loss: 0.2152\n",
            "Epoch 170/200 â€” Loss: 0.2144\n",
            "Epoch 171/200 â€” Loss: 0.2131\n",
            "Epoch 172/200 â€” Loss: 0.2125\n",
            "Epoch 173/200 â€” Loss: 0.2118\n",
            "Epoch 174/200 â€” Loss: 0.2121\n",
            "Epoch 175/200 â€” Loss: 0.2128\n",
            "Epoch 176/200 â€” Loss: 0.2120\n",
            "Epoch 177/200 â€” Loss: 0.2120\n",
            "Epoch 178/200 â€” Loss: 0.2107\n",
            "Epoch 179/200 â€” Loss: 0.2092\n",
            "Epoch 180/200 â€” Loss: 0.2091\n",
            "Epoch 181/200 â€” Loss: 0.2084\n",
            "Epoch 182/200 â€” Loss: 0.2081\n",
            "Epoch 183/200 â€” Loss: 0.2080\n",
            "Epoch 184/200 â€” Loss: 0.2083\n",
            "Epoch 185/200 â€” Loss: 0.2069\n",
            "Epoch 186/200 â€” Loss: 0.2071\n",
            "Epoch 187/200 â€” Loss: 0.2057\n",
            "Epoch 188/200 â€” Loss: 0.2057\n",
            "Epoch 189/200 â€” Loss: 0.2057\n",
            "Epoch 190/200 â€” Loss: 0.2053\n",
            "Epoch 191/200 â€” Loss: 0.2049\n",
            "Epoch 192/200 â€” Loss: 0.2047\n",
            "Epoch 193/200 â€” Loss: 0.2051\n",
            "Epoch 194/200 â€” Loss: 0.2041\n",
            "Epoch 195/200 â€” Loss: 0.2036\n",
            "Epoch 196/200 â€” Loss: 0.2035\n",
            "Epoch 197/200 â€” Loss: 0.2029\n",
            "Epoch 198/200 â€” Loss: 0.2020\n",
            "Epoch 199/200 â€” Loss: 0.2023\n",
            "Epoch 200/200 â€” Loss: 0.2023\n",
            "\n",
            "Training selesai.\n"
          ]
        }
      ],
      "source": [
        "print(\"Mulai training BPR...\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    iters_per_epoch = max(1, n_pos // batch_size)\n",
        "\n",
        "    # ============================\n",
        "    # Warmup Learning Rate\n",
        "    # ============================\n",
        "    if epoch <= warmup_epochs:\n",
        "        for g in optimizer.param_groups:\n",
        "            g['lr'] = lr * (epoch / warmup_epochs)\n",
        "\n",
        "    # ============================\n",
        "    # Training Loop per epoch\n",
        "    # ============================\n",
        "    for _ in range(iters_per_epoch):\n",
        "        users_b, pos_b, neg_b = sample_batch(batch_size)\n",
        "\n",
        "        u = torch.tensor(users_b, dtype=torch.long, device=device)\n",
        "        i = torch.tensor(pos_b, dtype=torch.long, device=device)\n",
        "        j = torch.tensor(neg_b, dtype=torch.long, device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_ui, x_uj = model(u, i, j)\n",
        "        x_diff = x_ui - x_uj\n",
        "\n",
        "        loss = nn.functional.softplus(-x_diff).mean()\n",
        "\n",
        "        reg_term = (\n",
        "            model.user_emb(u).norm(2).pow(2) +\n",
        "            model.item_emb(i).norm(2).pow(2) +\n",
        "            model.item_emb(j).norm(2).pow(2)\n",
        "        ) / batch_size\n",
        "\n",
        "        loss = loss + reg * reg_term.sum()\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}/{epochs} â€” Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining selesai.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "02BKvzIdbqKy"
      },
      "outputs": [],
      "source": [
        "# Tahap 8: Rekomendasi\n",
        "user_embeddings = model.user_emb.weight.detach().cpu().numpy()\n",
        "item_embeddings = model.item_emb.weight.detach().cpu().numpy()\n",
        "\n",
        "def recommend_bpr_topk_for_user(u_idx, top_k=10):\n",
        "    u_vec = user_embeddings[u_idx]\n",
        "    scores = item_embeddings.dot(u_vec)\n",
        "\n",
        "    if u_idx in user_pos:\n",
        "        scores[list(user_pos[u_idx])] = -1e12\n",
        "\n",
        "    topk_idx = np.argpartition(-scores, top_k)[:top_k]\n",
        "    topk_sorted = topk_idx[np.argsort(-scores[topk_idx])]\n",
        "    return [index_to_item[i] for i in topk_sorted]\n",
        "\n",
        "# Generate rekomendasi\n",
        "bpr_results = {}\n",
        "for uid in test_truth:\n",
        "    if uid in user_to_index:\n",
        "        u = user_to_index[uid]\n",
        "        bpr_results[uid] = recommend_bpr_topk_for_user(u, top_k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QplQu5-7sDfr",
        "outputId": "0820aa36-faa3-4d5f-cb75-a89ae5d4a0da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAP@10 BPR: 0.013983759013626163\n",
            "Submission saved: bpr_submission.csv\n"
          ]
        }
      ],
      "source": [
        "# Tahap 9: Evaluasi MAP@10\n",
        "def average_precision_at_k(actual, predicted, k=10):\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "    predicted = predicted[:k]\n",
        "    score = 0.0\n",
        "    hits = 0.0\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual:\n",
        "            hits += 1.0\n",
        "            score += hits / (i + 1.0)\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "test_truth = test_split.groupby('user_id')['item_id'].apply(set).to_dict()\n",
        "\n",
        "bpr_results = {}\n",
        "for uid in test_truth:\n",
        "    if uid in user_to_index:\n",
        "        bpr_results[uid] = recommend_bpr_topk_for_user(user_to_index[uid], top_k=10)\n",
        "\n",
        "scores = []\n",
        "for uid, actual in test_truth.items():\n",
        "    preds = bpr_results.get(uid, [])\n",
        "    ap = average_precision_at_k(actual, preds, k=10)\n",
        "    scores.append(ap)\n",
        "\n",
        "bpr_map10 = np.mean(scores)\n",
        "print(\"MAP@10 BPR:\", bpr_map10)\n",
        "\n",
        "\n",
        "global_top10 = train_split['item_id'].value_counts().head(10).index.tolist()\n",
        "\n",
        "out_users = sorted(user_to_index.keys())\n",
        "\n",
        "rows = []\n",
        "for uid in out_users:\n",
        "    recs = bpr_results.get(uid, global_top10)\n",
        "    rows.append({'user_id': uid, 'item_id': \" \".join(map(str, recs))})\n",
        "\n",
        "sub_bpr = pd.DataFrame(rows)\n",
        "sub_bpr.to_csv(\"bpr_submission.csv\", index=False)\n",
        "\n",
        "print(\"Submission saved: bpr_submission.csv\")\n",
        "pd.read_csv(\"bpr_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fine_tune_bpr(n_users, n_items, train_data, test_data, user_item_matrix):\n",
        "\n",
        "    param_grid = {\n",
        "        \"embedding_dim\": [32, 64, 128],\n",
        "        \"lr\": [0.0005, 0.001, 0.005],\n",
        "        \"lambda_reg\": [1e-4, 1e-5],\n",
        "        \"num_negative\": [1, 3, 5]\n",
        "    }\n",
        "\n",
        "    best_auc = -1\n",
        "    best_params = None\n",
        "\n",
        "    for emb in param_grid[\"embedding_dim\"]:\n",
        "        for lr in param_grid[\"lr\"]:\n",
        "            for reg in param_grid[\"lambda_reg\"]:\n",
        "                for neg in param_grid[\"num_negative\"]:\n",
        "\n",
        "                    print(f\"\\nðŸ”§ Testing: emb={emb}, lr={lr}, reg={reg}, neg={neg}\")\n",
        "\n",
        "                    model = BPR(n_users, n_items, embedding_dim=emb)\n",
        "                    train_bpr(\n",
        "                        model, train_data, user_item_matrix,\n",
        "                        lr=lr, lambda_reg=reg, num_negative=neg, epochs=5\n",
        "                    )\n",
        "\n",
        "                    auc = auc_score(model, test_data)\n",
        "                    print(f\"âž¡ AUC: {auc:.4f}\")\n",
        "\n",
        "                    if auc > best_auc:\n",
        "                        best_auc = auc\n",
        "                        best_params = {\n",
        "                            \"embedding_dim\": emb,\n",
        "                            \"lr\": lr,\n",
        "                            \"lambda_reg\": reg,\n",
        "                            \"num_negative\": neg\n",
        "                        }\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"ðŸ”¥ BEST HYPERPARAMETERS FOUND\")\n",
        "    print(best_params)\n",
        "    print(f\"Best AUC = {best_auc:.4f}\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    return best_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_params = fine_tune_bpr(\n",
        "    n_users, \n",
        "    n_items, \n",
        "    train_data, \n",
        "    test_data, \n",
        "    user_item_matrix\n",
        ")\n",
        "\n",
        "best_model = BPR(n_users, n_items, embedding_dim=best_params[\"embedding_dim\"])\n",
        "\n",
        "train_bpr(\n",
        "    best_model, train_data, user_item_matrix,\n",
        "    lr=best_params[\"lr\"],\n",
        "    lambda_reg=best_params[\"lambda_reg\"],\n",
        "    num_negative=best_params[\"num_negative\"],\n",
        "    epochs=30\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_precision_at_k(actual, predicted, k=10):\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "    predicted = predicted[:k]\n",
        "    score = 0.0\n",
        "    hits = 0.0\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual:\n",
        "            hits += 1.0\n",
        "            score += hits / (i + 1.0)\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "def recommend_bpr_topk_for_user_tuned(u_idx, user_emb_weights, item_emb_weights, user_positive_items, index_to_item_map, top_k=10):\n",
        "    u_vec = user_emb_weights[u_idx]\n",
        "    scores = item_emb_weights.dot(u_vec)\n",
        "\n",
        "    if u_idx in user_positive_items:\n",
        "        scores[list(user_positive_items[u_idx])] = -1e12 # Mask already interacted items\n",
        "\n",
        "    topk_idx = np.argpartition(-scores, top_k)[:top_k]\n",
        "    topk_sorted = topk_idx[np.argsort(-scores[topk_idx])]\n",
        "    return [index_to_item_map[i] for i in topk_sorted]\n",
        "\n",
        "def evaluate_map_at_k(model, test_df, user_to_idx, item_to_idx, idx_to_item, user_pos_dict, top_k=10):\n",
        "    user_embeddings = model.user_emb.weight.detach().cpu().numpy()\n",
        "    item_embeddings = model.item_emb.weight.detach().cpu().numpy()\n",
        "\n",
        "    test_truth = test_df.groupby('user_id')['item_id'].apply(set).to_dict()\n",
        "    scores_list = []\n",
        "\n",
        "    for uid, actual_items in test_truth.items():\n",
        "        if uid in user_to_idx:\n",
        "            u_idx = user_to_idx[uid]\n",
        "            preds = recommend_bpr_topk_for_user_tuned(u_idx, user_embeddings, item_embeddings, user_pos_dict, idx_to_item, top_k=top_k)\n",
        "            ap = average_precision_at_k(actual_items, preds, k=top_k)\n",
        "            scores_list.append(ap)\n",
        "    return np.mean(scores_list) if scores_list else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tahap 10: Hyperparameter Tuning for BPR\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def fine_tune_bpr(n_users, n_items, train_df, test_df, user_to_idx, item_to_idx, idx_to_user, idx_to_item,\n",
        "                  user_pos_dict, pos_pairs_arr, popular_item_indices_arr, item_prob_np_arr, n_pos_interactions,\n",
        "                  device_obj, batch_size, epochs_per_trial=5, eval_top_k=10):\n",
        "\n",
        "    param_grid = {\n",
        "        \"n_factors\": [32, 64, 128], # Embedding dimensions\n",
        "        \"lr\": [0.0005, 0.001, 0.005],\n",
        "        \"reg\": [1e-4, 1e-5],        # Regularization strength\n",
        "        \"neg_samples\": [1, 3, 5]    # Number of negative samples per positive\n",
        "    }\n",
        "\n",
        "    best_map_score = -1\n",
        "    best_params = None\n",
        "\n",
        "    all_param_combinations = []\n",
        "    for f in param_grid[\"n_factors\"]:\n",
        "        for l in param_grid[\"lr\"]:\n",
        "            for r in param_grid[\"reg\"]:\n",
        "                for n in param_grid[\"neg_samples\"]:\n",
        "                    all_param_combinations.append({\"n_factors\": f, \"lr\": l, \"reg\": r, \"neg_samples\": n})\n",
        "\n",
        "    for params in tqdm(all_param_combinations, desc=\"Tuning Hyperparameters\"):\n",
        "        n_factors = params[\"n_factors\"]\n",
        "        current_lr = params[\"lr\"]\n",
        "        current_reg = params[\"reg\"]\n",
        "        current_neg_samples = params[\"neg_samples\"]\n",
        "\n",
        "        print(f\"\\nðŸ”§ Testing: Factors={n_factors}, LR={current_lr}, Reg={current_reg}, NegSamples={current_neg_samples}\")\n",
        "\n",
        "        model = BPRModel(n_users, n_items, n_factors=n_factors).to(device_obj)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=current_lr)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs_per_trial)\n",
        "\n",
        "        # Inner training loop for hyperparameter tuning\n",
        "        for epoch in range(1, epochs_per_trial + 1):\n",
        "            epoch_loss = 0\n",
        "            iters_per_epoch = max(1, n_pos_interactions // batch_size)\n",
        "\n",
        "            for _ in range(iters_per_epoch):\n",
        "                # Sample batch logic adapted for current_neg_samples\n",
        "                idx = np.random.randint(0, n_pos_interactions, size=batch_size)\n",
        "                batch = pos_pairs_arr[idx]\n",
        "\n",
        "                users_b = batch[:, 0]\n",
        "                pos_b = batch[:, 1]\n",
        "\n",
        "                neg_items = np.random.choice(\n",
        "                    popular_item_indices_arr,\n",
        "                    size=batch_size * current_neg_samples,\n",
        "                    p=item_prob_np_arr\n",
        "                ).reshape(batch_size, current_neg_samples)\n",
        "\n",
        "                for b in range(batch_size):\n",
        "                    u = users_b[b]\n",
        "                    for k in range(current_neg_samples):\n",
        "                        neg = neg_items[b, k]\n",
        "                        attempt = 0\n",
        "                        while neg in user_pos_dict[u] and attempt < 10: # Avoid sampling positive items as negative\n",
        "                            neg = np.random.choice(popular_item_indices_arr, p=item_prob_np_arr)\n",
        "                            attempt += 1\n",
        "                        neg_items[b, k] = neg\n",
        "\n",
        "                u_tensor = torch.tensor(users_b, dtype=torch.long, device=device_obj)\n",
        "                i_tensor = torch.tensor(pos_b, dtype=torch.long, device=device_obj)\n",
        "                j_tensor = torch.tensor(neg_items[:, 0], dtype=torch.long, device=device_obj) # Use only first negative sample for BPR loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                x_ui, x_uj = model(u_tensor, i_tensor, j_tensor)\n",
        "                x_diff = x_ui - x_uj\n",
        "\n",
        "                loss = nn.functional.softplus(-x_diff).mean()\n",
        "\n",
        "                reg_term = (\n",
        "                    model.user_emb(u_tensor).norm(2).pow(2) +\n",
        "                    model.item_emb(i_tensor).norm(2).pow(2) +\n",
        "                    model.item_emb(j_tensor).norm(2).pow(2)\n",
        "                ) / batch_size\n",
        "\n",
        "                loss = loss + current_reg * reg_term.sum()\n",
        "\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            scheduler.step() # Scheduler step per epoch\n",
        "\n",
        "        # Evaluation\n",
        "        current_map = evaluate_map_at_k(model, test_df, user_to_idx, item_to_idx, idx_to_item, user_pos_dict, eval_top_k)\n",
        "        print(f\"âž¡ MAP@{eval_top_k}: {current_map:.4f}\")\n",
        "\n",
        "        if current_map > best_map_score:\n",
        "            best_map_score = current_map\n",
        "            best_params = params\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"ðŸ”¥ BEST HYPERPARAMETERS FOUND\")\n",
        "    print(best_params)\n",
        "    print(f\"Best MAP@{eval_top_k} = {best_map_score:.4f}\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    return best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_params = fine_tune_bpr(\n",
        "    n_users=n_users,\n",
        "    n_items=n_items,\n",
        "    train_df=train_split,\n",
        "    test_df=test_split,\n",
        "    user_to_idx=user_to_index,\n",
        "    item_to_idx=item_to_index,\n",
        "    idx_to_user=index_to_user,\n",
        "    idx_to_item=index_to_item,\n",
        "    user_pos_dict=user_pos,\n",
        "    pos_pairs_arr=pos_pairs_array,\n",
        "    popular_item_indices_arr=popular_item_indices,\n",
        "    item_prob_np_arr=item_prob_np,\n",
        "    n_pos_interactions=n_pos,\n",
        "    device_obj=device,\n",
        "    batch_size=batch_size,\n",
        "    epochs_per_trial=5,\n",
        "    eval_top_k=10\n",
        ")\n",
        "\n",
        "# After finding best_params, train the final model with these parameters for more epochs\n",
        "best_model = BPRModel(n_users, n_items, n_factors=best_params[\"n_factors\"]).to(device)\n",
        "optimizer = optim.Adam(best_model.parameters(), lr=best_params[\"lr\"])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30) # Train for more epochs\n",
        "\n",
        "# Final training loop for the best model\n",
        "print(\"\\nMulai training final BPR dengan hyperparameter terbaik...\")\n",
        "for epoch in range(1, 30 + 1):\n",
        "    epoch_loss = 0\n",
        "    iters_per_epoch = max(1, n_pos // batch_size)\n",
        "\n",
        "    for _ in range(iters_per_epoch):\n",
        "        # Sample batch logic from sample_batch function\n",
        "        idx = np.random.randint(0, n_pos, size=batch_size)\n",
        "        batch = pos_pairs_array[idx]\n",
        "\n",
        "        users_b = batch[:, 0]\n",
        "        pos_b = batch[:, 1]\n",
        "\n",
        "        neg_items = np.random.choice(\n",
        "            popular_item_indices,\n",
        "            size=batch_size * best_params[\"neg_samples\"],\n",
        "            p=item_prob_np\n",
        "        ).reshape(batch_size, best_params[\"neg_samples\"])\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            u = users_b[b]\n",
        "            for k in range(best_params[\"neg_samples\"]):\n",
        "                neg = neg_items[b, k]\n",
        "                attempt = 0\n",
        "                while neg in user_pos[u] and attempt < 10:\n",
        "                    neg = np.random.choice(popular_item_indices, p=item_prob_np)\n",
        "                    attempt += 1\n",
        "                neg_items[b, k] = neg\n",
        "\n",
        "        u_tensor = torch.tensor(users_b, dtype=torch.long, device=device)\n",
        "        i_tensor = torch.tensor(pos_b, dtype=torch.long, device=device)\n",
        "        j_tensor = torch.tensor(neg_items[:, 0], dtype=torch.long, device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_ui, x_uj = best_model(u_tensor, i_tensor, j_tensor)\n",
        "        x_diff = x_ui - x_uj\n",
        "\n",
        "        loss = nn.functional.softplus(-x_diff).mean()\n",
        "\n",
        "        reg_term = (\n",
        "            best_model.user_emb(u_tensor).norm(2).pow(2) +\n",
        "            best_model.item_emb(i_tensor).norm(2).pow(2) +\n",
        "            best_model.item_emb(j_tensor).norm(2).pow(2)\n",
        "        ) / batch_size\n",
        "\n",
        "        loss = loss + best_params[\"reg\"] * reg_term.sum()\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(best_model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch}/30 â€” Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(\"Training final model selesai.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
