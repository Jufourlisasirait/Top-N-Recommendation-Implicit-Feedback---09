{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CkfLOvwT_R8",
        "outputId": "b89ec261-a934-4f76-81ab-fc1672c9f5c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total interactions (after dropping NaNs): 239375 (Dropped 1 rows)\n"
          ]
        }
      ],
      "source": [
        "# Tahap 1: Load data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Baca file CSV\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Hapus baris dengan nilai NaN di 'item_id'\n",
        "initial_len = len(train)\n",
        "train.dropna(subset=['item_id'], inplace=True)\n",
        "print(f\"Total interactions (after dropping NaNs): {len(train)} (Dropped {initial_len - len(train)} rows)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy25ywhqXBqp",
        "outputId": "806eb34f-d5a5-4a6d-ef1a-f36f17d7bb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 187052, Test: 52323\n"
          ]
        }
      ],
      "source": [
        "# Tahap 2: Split Data jadi train-test per user\n",
        "train_list, test_list = [], []\n",
        "\n",
        "for uid, group in train.groupby(\"user_id\"):\n",
        "    if len(group) < 2:\n",
        "        train_list.append(group)\n",
        "        continue\n",
        "    tr, te = train_test_split(group, test_size=0.2, random_state=42)\n",
        "    train_list.append(tr)\n",
        "    test_list.append(te)\n",
        "\n",
        "train_split = pd.concat(train_list)\n",
        "test_split = pd.concat(test_list)\n",
        "print(f\"Train: {len(train_split)}, Test: {len(test_split)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YIjrxgSzJII",
        "outputId": "75df15b1-4bc4-416b-8bda-6a6876703d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Users: 12160, Items: 95211\n"
          ]
        }
      ],
      "source": [
        "# Tahap 3: Encode user & item ke index numerik\n",
        "user_to_index = {u: i for i, u in enumerate(train_split['user_id'].unique())}\n",
        "item_to_index = {i: j for j, i in enumerate(train_split['item_id'].unique())}\n",
        "index_to_user = {i: u for u, i in user_to_index.items()}\n",
        "index_to_item = {j: i for i, j in item_to_index.items()}\n",
        "\n",
        "user_index = train_split['user_id'].map(user_to_index)\n",
        "item_index = train_split['item_id'].map(item_to_index)\n",
        "\n",
        "n_users = len(user_to_index)\n",
        "n_items = len(item_to_index)\n",
        "print(f\"Users: {n_users}, Items: {n_items}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEvDBBrdXC9t",
        "outputId": "d2d706d3-dfbd-488b-c929-9c7b0f88a18c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSR matrix shape: (12160, 95211)\n"
          ]
        }
      ],
      "source": [
        "# Tahap 4: Buat weighted user-item matrix (TF-IDF style)\n",
        "item_freq = train_split['item_id'].value_counts()\n",
        "item_weight = 1.0 / np.log1p(item_freq)  # Semakin populer, bobot makin kecil\n",
        "train_split['weight'] = train_split['item_id'].map(item_weight)\n",
        "\n",
        "data = train_split['weight'].values.astype(np.float32)\n",
        "\n",
        "user_item_csr = csr_matrix((data, (user_index, item_index)), shape=(n_users, n_items))\n",
        "print(\"CSR matrix shape:\", user_item_csr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "gNdXHfGtPxI1"
      },
      "outputs": [],
      "source": [
        "# Tahap 5: Persiapan Data untuk BPR\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Membentuk pasangan positif (user_idx, item_idx)\n",
        "pos_pairs = list(\n",
        "    zip(train_split['user_id'].map(user_to_index),\n",
        "        train_split['item_id'].map(item_to_index))\n",
        ")\n",
        "\n",
        "# Set item yang pernah di-like per user\n",
        "user_pos = defaultdict(set)\n",
        "for u, i in pos_pairs:\n",
        "    user_pos[u].add(i)\n",
        "\n",
        "# Hitung frekuensi item untuk popularity negative sampling\n",
        "item_freq_series = train_split['item_id'].map(item_to_index).value_counts()\n",
        "item_freq = np.zeros(n_items, dtype=np.float32)\n",
        "for idx, cnt in item_freq_series.items():\n",
        "    item_freq[idx] = cnt\n",
        "\n",
        "# Distribusi peluang sampling (power 0.75)\n",
        "power = 0.75\n",
        "freq = item_freq / item_freq.sum()\n",
        "item_prob_np = freq ** 0.75\n",
        "item_prob_np = item_prob_np / item_prob_np.sum()\n",
        "popular_item_indices = np.arange(n_items)\n",
        "pos_pairs_array = np.array(pos_pairs, dtype=np.int32)\n",
        "n_pos = len(pos_pairs_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "i_FDOzP7QAkg"
      },
      "outputs": [],
      "source": [
        "# Tahap 6: Defenisi Model BPR\n",
        "class BPRModel(nn.Module):\n",
        "    def __init__(self, n_users, n_items, n_factors=128):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, n_factors)\n",
        "        self.item_emb = nn.Embedding(n_items, n_factors)\n",
        "\n",
        "        nn.init.normal_(self.user_emb.weight, std=0.01)\n",
        "        nn.init.normal_(self.item_emb.weight, std=0.01)\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        u_vec = self.user_emb(u)\n",
        "        i_vec = self.item_emb(i)\n",
        "        j_vec = self.item_emb(j)\n",
        "        x_ui = torch.sum(u_vec * i_vec, dim=1)\n",
        "        x_uj = torch.sum(u_vec * j_vec, dim=1)\n",
        "        return x_ui, x_uj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "wC4y2CC6QE9d"
      },
      "outputs": [],
      "source": [
        "# Tahap 7:Training BPR\n",
        "factors = 256\n",
        "lr = 0.001\n",
        "warmup_epochs = 5\n",
        "batch_size = 4096\n",
        "neg_samples = 2\n",
        "epochs = 200\n",
        "reg = 1e-4\n",
        "\n",
        "model = BPRModel(n_users, n_items, n_factors=factors).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "def sample_batch(batch_size):\n",
        "    idx = np.random.randint(0, n_pos, size=batch_size)\n",
        "    batch = pos_pairs_array[idx]\n",
        "\n",
        "    users = batch[:, 0]\n",
        "    pos_items = batch[:, 1]\n",
        "\n",
        "    neg_items = np.random.choice(\n",
        "        popular_item_indices,\n",
        "        size=batch_size * neg_samples,\n",
        "        p=item_prob_np\n",
        "    ).reshape(batch_size, neg_samples)\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        u = users[b]\n",
        "        for k in range(neg_samples):\n",
        "            neg = neg_items[b, k]\n",
        "            attempt = 0\n",
        "            while neg in user_pos[u] and attempt < 10:\n",
        "                neg = np.random.choice(popular_item_indices, p=item_prob_np)\n",
        "                attempt += 1\n",
        "            neg_items[b, k] = neg\n",
        "\n",
        "    return users.astype(np.int64), pos_items.astype(np.int64), neg_items[:, 0].astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o81nHKQjhuu",
        "outputId": "b5dbd7d1-03dc-4c2c-9894-c2bf10269fad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mulai training BPR...\n",
            "\n",
            "Epoch 1/200 — Loss: 31.1782\n",
            "Epoch 2/200 — Loss: 31.1231\n",
            "Epoch 3/200 — Loss: 30.9959\n",
            "Epoch 4/200 — Loss: 30.7270\n",
            "Epoch 5/200 — Loss: 30.4555\n",
            "Epoch 6/200 — Loss: 30.3687\n",
            "Epoch 7/200 — Loss: 30.0013\n",
            "Epoch 8/200 — Loss: 28.8662\n",
            "Epoch 9/200 — Loss: 26.4299\n",
            "Epoch 10/200 — Loss: 23.0468\n",
            "Epoch 11/200 — Loss: 19.8981\n",
            "Epoch 12/200 — Loss: 17.9608\n",
            "Epoch 13/200 — Loss: 17.2047\n",
            "Epoch 14/200 — Loss: 17.0878\n",
            "Epoch 15/200 — Loss: 16.8902\n",
            "Epoch 16/200 — Loss: 16.0490\n",
            "Epoch 17/200 — Loss: 14.3193\n",
            "Epoch 18/200 — Loss: 12.2486\n",
            "Epoch 19/200 — Loss: 10.3091\n",
            "Epoch 20/200 — Loss: 8.9200\n",
            "Epoch 21/200 — Loss: 8.1583\n",
            "Epoch 22/200 — Loss: 7.9076\n",
            "Epoch 23/200 — Loss: 7.8626\n",
            "Epoch 24/200 — Loss: 7.7297\n",
            "Epoch 25/200 — Loss: 7.2763\n",
            "Epoch 26/200 — Loss: 6.4751\n",
            "Epoch 27/200 — Loss: 5.5723\n",
            "Epoch 28/200 — Loss: 4.7650\n",
            "Epoch 29/200 — Loss: 4.2128\n",
            "Epoch 30/200 — Loss: 3.9305\n",
            "Epoch 31/200 — Loss: 3.8267\n",
            "Epoch 32/200 — Loss: 3.8160\n",
            "Epoch 33/200 — Loss: 3.7523\n",
            "Epoch 34/200 — Loss: 3.5532\n",
            "Epoch 35/200 — Loss: 3.1929\n",
            "Epoch 36/200 — Loss: 2.7926\n",
            "Epoch 37/200 — Loss: 2.4738\n",
            "Epoch 38/200 — Loss: 2.2477\n",
            "Epoch 39/200 — Loss: 2.1221\n",
            "Epoch 40/200 — Loss: 2.0934\n",
            "Epoch 41/200 — Loss: 2.0822\n",
            "Epoch 42/200 — Loss: 2.0501\n",
            "Epoch 43/200 — Loss: 1.9563\n",
            "Epoch 44/200 — Loss: 1.7906\n",
            "Epoch 45/200 — Loss: 1.6078\n",
            "Epoch 46/200 — Loss: 1.4485\n",
            "Epoch 47/200 — Loss: 1.3491\n",
            "Epoch 48/200 — Loss: 1.3077\n",
            "Epoch 49/200 — Loss: 1.2883\n",
            "Epoch 50/200 — Loss: 1.2873\n",
            "Epoch 51/200 — Loss: 1.2608\n",
            "Epoch 52/200 — Loss: 1.2187\n",
            "Epoch 53/200 — Loss: 1.1216\n",
            "Epoch 54/200 — Loss: 1.0331\n",
            "Epoch 55/200 — Loss: 0.9600\n",
            "Epoch 56/200 — Loss: 0.9086\n",
            "Epoch 57/200 — Loss: 0.8807\n",
            "Epoch 58/200 — Loss: 0.8785\n",
            "Epoch 59/200 — Loss: 0.8754\n",
            "Epoch 60/200 — Loss: 0.8596\n",
            "Epoch 61/200 — Loss: 0.8305\n",
            "Epoch 62/200 — Loss: 0.7828\n",
            "Epoch 63/200 — Loss: 0.7300\n",
            "Epoch 64/200 — Loss: 0.6881\n",
            "Epoch 65/200 — Loss: 0.6601\n",
            "Epoch 66/200 — Loss: 0.6465\n",
            "Epoch 67/200 — Loss: 0.6454\n",
            "Epoch 68/200 — Loss: 0.6431\n",
            "Epoch 69/200 — Loss: 0.6322\n",
            "Epoch 70/200 — Loss: 0.6196\n",
            "Epoch 71/200 — Loss: 0.5816\n",
            "Epoch 72/200 — Loss: 0.5531\n",
            "Epoch 73/200 — Loss: 0.5301\n",
            "Epoch 74/200 — Loss: 0.5162\n",
            "Epoch 75/200 — Loss: 0.5043\n",
            "Epoch 76/200 — Loss: 0.5060\n",
            "Epoch 77/200 — Loss: 0.5044\n",
            "Epoch 78/200 — Loss: 0.4971\n",
            "Epoch 79/200 — Loss: 0.4820\n",
            "Epoch 80/200 — Loss: 0.4675\n",
            "Epoch 81/200 — Loss: 0.4478\n",
            "Epoch 82/200 — Loss: 0.4327\n",
            "Epoch 83/200 — Loss: 0.4203\n",
            "Epoch 84/200 — Loss: 0.4157\n",
            "Epoch 85/200 — Loss: 0.4176\n",
            "Epoch 86/200 — Loss: 0.4174\n",
            "Epoch 87/200 — Loss: 0.4133\n",
            "Epoch 88/200 — Loss: 0.4029\n",
            "Epoch 89/200 — Loss: 0.3890\n",
            "Epoch 90/200 — Loss: 0.3804\n",
            "Epoch 91/200 — Loss: 0.3660\n",
            "Epoch 92/200 — Loss: 0.3600\n",
            "Epoch 93/200 — Loss: 0.3586\n",
            "Epoch 94/200 — Loss: 0.3584\n",
            "Epoch 95/200 — Loss: 0.3558\n",
            "Epoch 96/200 — Loss: 0.3544\n",
            "Epoch 97/200 — Loss: 0.3462\n",
            "Epoch 98/200 — Loss: 0.3404\n",
            "Epoch 99/200 — Loss: 0.3289\n",
            "Epoch 100/200 — Loss: 0.3231\n",
            "Epoch 101/200 — Loss: 0.3195\n",
            "Epoch 102/200 — Loss: 0.3178\n",
            "Epoch 103/200 — Loss: 0.3173\n",
            "Epoch 104/200 — Loss: 0.3152\n",
            "Epoch 105/200 — Loss: 0.3132\n",
            "Epoch 106/200 — Loss: 0.3093\n",
            "Epoch 107/200 — Loss: 0.3011\n",
            "Epoch 108/200 — Loss: 0.2975\n",
            "Epoch 109/200 — Loss: 0.2903\n",
            "Epoch 110/200 — Loss: 0.2888\n",
            "Epoch 111/200 — Loss: 0.2883\n",
            "Epoch 112/200 — Loss: 0.2877\n",
            "Epoch 113/200 — Loss: 0.2879\n",
            "Epoch 114/200 — Loss: 0.2857\n",
            "Epoch 115/200 — Loss: 0.2824\n",
            "Epoch 116/200 — Loss: 0.2765\n",
            "Epoch 117/200 — Loss: 0.2731\n",
            "Epoch 118/200 — Loss: 0.2703\n",
            "Epoch 119/200 — Loss: 0.2681\n",
            "Epoch 120/200 — Loss: 0.2667\n",
            "Epoch 121/200 — Loss: 0.2658\n",
            "Epoch 122/200 — Loss: 0.2657\n",
            "Epoch 123/200 — Loss: 0.2640\n",
            "Epoch 124/200 — Loss: 0.2618\n",
            "Epoch 125/200 — Loss: 0.2575\n",
            "Epoch 126/200 — Loss: 0.2562\n",
            "Epoch 127/200 — Loss: 0.2530\n",
            "Epoch 128/200 — Loss: 0.2523\n",
            "Epoch 129/200 — Loss: 0.2517\n",
            "Epoch 130/200 — Loss: 0.2508\n",
            "Epoch 131/200 — Loss: 0.2507\n",
            "Epoch 132/200 — Loss: 0.2501\n",
            "Epoch 133/200 — Loss: 0.2480\n",
            "Epoch 134/200 — Loss: 0.2441\n",
            "Epoch 135/200 — Loss: 0.2433\n",
            "Epoch 136/200 — Loss: 0.2412\n",
            "Epoch 137/200 — Loss: 0.2397\n",
            "Epoch 138/200 — Loss: 0.2401\n",
            "Epoch 139/200 — Loss: 0.2383\n",
            "Epoch 140/200 — Loss: 0.2388\n",
            "Epoch 141/200 — Loss: 0.2372\n",
            "Epoch 142/200 — Loss: 0.2370\n",
            "Epoch 143/200 — Loss: 0.2340\n",
            "Epoch 144/200 — Loss: 0.2317\n",
            "Epoch 145/200 — Loss: 0.2317\n",
            "Epoch 146/200 — Loss: 0.2302\n",
            "Epoch 147/200 — Loss: 0.2312\n",
            "Epoch 148/200 — Loss: 0.2294\n",
            "Epoch 149/200 — Loss: 0.2300\n",
            "Epoch 150/200 — Loss: 0.2303\n",
            "Epoch 151/200 — Loss: 0.2272\n",
            "Epoch 152/200 — Loss: 0.2266\n",
            "Epoch 153/200 — Loss: 0.2247\n",
            "Epoch 154/200 — Loss: 0.2237\n",
            "Epoch 155/200 — Loss: 0.2230\n",
            "Epoch 156/200 — Loss: 0.2231\n",
            "Epoch 157/200 — Loss: 0.2227\n",
            "Epoch 158/200 — Loss: 0.2237\n",
            "Epoch 159/200 — Loss: 0.2219\n",
            "Epoch 160/200 — Loss: 0.2210\n",
            "Epoch 161/200 — Loss: 0.2194\n",
            "Epoch 162/200 — Loss: 0.2178\n",
            "Epoch 163/200 — Loss: 0.2176\n",
            "Epoch 164/200 — Loss: 0.2173\n",
            "Epoch 165/200 — Loss: 0.2162\n",
            "Epoch 166/200 — Loss: 0.2169\n",
            "Epoch 167/200 — Loss: 0.2165\n",
            "Epoch 168/200 — Loss: 0.2160\n",
            "Epoch 169/200 — Loss: 0.2152\n",
            "Epoch 170/200 — Loss: 0.2144\n",
            "Epoch 171/200 — Loss: 0.2131\n",
            "Epoch 172/200 — Loss: 0.2125\n",
            "Epoch 173/200 — Loss: 0.2118\n",
            "Epoch 174/200 — Loss: 0.2121\n",
            "Epoch 175/200 — Loss: 0.2128\n",
            "Epoch 176/200 — Loss: 0.2120\n",
            "Epoch 177/200 — Loss: 0.2120\n",
            "Epoch 178/200 — Loss: 0.2107\n",
            "Epoch 179/200 — Loss: 0.2092\n",
            "Epoch 180/200 — Loss: 0.2091\n",
            "Epoch 181/200 — Loss: 0.2084\n",
            "Epoch 182/200 — Loss: 0.2081\n",
            "Epoch 183/200 — Loss: 0.2080\n",
            "Epoch 184/200 — Loss: 0.2083\n",
            "Epoch 185/200 — Loss: 0.2069\n",
            "Epoch 186/200 — Loss: 0.2071\n",
            "Epoch 187/200 — Loss: 0.2057\n",
            "Epoch 188/200 — Loss: 0.2057\n",
            "Epoch 189/200 — Loss: 0.2057\n",
            "Epoch 190/200 — Loss: 0.2053\n",
            "Epoch 191/200 — Loss: 0.2049\n",
            "Epoch 192/200 — Loss: 0.2047\n",
            "Epoch 193/200 — Loss: 0.2051\n",
            "Epoch 194/200 — Loss: 0.2041\n",
            "Epoch 195/200 — Loss: 0.2036\n",
            "Epoch 196/200 — Loss: 0.2035\n",
            "Epoch 197/200 — Loss: 0.2029\n",
            "Epoch 198/200 — Loss: 0.2020\n",
            "Epoch 199/200 — Loss: 0.2023\n",
            "Epoch 200/200 — Loss: 0.2023\n",
            "\n",
            "Training selesai.\n"
          ]
        }
      ],
      "source": [
        "print(\"Mulai training BPR...\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    iters_per_epoch = max(1, n_pos // batch_size)\n",
        "\n",
        "    # ============================\n",
        "    # Warmup Learning Rate\n",
        "    # ============================\n",
        "    if epoch <= warmup_epochs:\n",
        "        for g in optimizer.param_groups:\n",
        "            g['lr'] = lr * (epoch / warmup_epochs)\n",
        "\n",
        "    # ============================\n",
        "    # Training Loop per epoch\n",
        "    # ============================\n",
        "    for _ in range(iters_per_epoch):\n",
        "        users_b, pos_b, neg_b = sample_batch(batch_size)\n",
        "\n",
        "        u = torch.tensor(users_b, dtype=torch.long, device=device)\n",
        "        i = torch.tensor(pos_b, dtype=torch.long, device=device)\n",
        "        j = torch.tensor(neg_b, dtype=torch.long, device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_ui, x_uj = model(u, i, j)\n",
        "        x_diff = x_ui - x_uj\n",
        "\n",
        "        loss = nn.functional.softplus(-x_diff).mean()\n",
        "\n",
        "        reg_term = (\n",
        "            model.user_emb(u).norm(2).pow(2) +\n",
        "            model.item_emb(i).norm(2).pow(2) +\n",
        "            model.item_emb(j).norm(2).pow(2)\n",
        "        ) / batch_size\n",
        "\n",
        "        loss = loss + reg * reg_term.sum()\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}/{epochs} — Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining selesai.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "02BKvzIdbqKy"
      },
      "outputs": [],
      "source": [
        "# Tahap 8: Rekomendasi\n",
        "user_embeddings = model.user_emb.weight.detach().cpu().numpy()\n",
        "item_embeddings = model.item_emb.weight.detach().cpu().numpy()\n",
        "\n",
        "def recommend_bpr_topk_for_user(u_idx, top_k=10):\n",
        "    u_vec = user_embeddings[u_idx]\n",
        "    scores = item_embeddings.dot(u_vec)\n",
        "\n",
        "    if u_idx in user_pos:\n",
        "        scores[list(user_pos[u_idx])] = -1e12\n",
        "\n",
        "    topk_idx = np.argpartition(-scores, top_k)[:top_k]\n",
        "    topk_sorted = topk_idx[np.argsort(-scores[topk_idx])]\n",
        "    return [index_to_item[i] for i in topk_sorted]\n",
        "\n",
        "# Generate rekomendasi\n",
        "bpr_results = {}\n",
        "for uid in test_truth:\n",
        "    if uid in user_to_index:\n",
        "        u = user_to_index[uid]\n",
        "        bpr_results[uid] = recommend_bpr_topk_for_user(u, top_k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QplQu5-7sDfr",
        "outputId": "0820aa36-faa3-4d5f-cb75-a89ae5d4a0da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAP@10 BPR: 0.013983759013626163\n",
            "Submission saved: bpr_submission.csv\n"
          ]
        }
      ],
      "source": [
        "# Tahap 9: Evaluasi MAP@10\n",
        "def average_precision_at_k(actual, predicted, k=10):\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "    predicted = predicted[:k]\n",
        "    score = 0.0\n",
        "    hits = 0.0\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual:\n",
        "            hits += 1.0\n",
        "            score += hits / (i + 1.0)\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "test_truth = test_split.groupby('user_id')['item_id'].apply(set).to_dict()\n",
        "\n",
        "bpr_results = {}\n",
        "for uid in test_truth:\n",
        "    if uid in user_to_index:\n",
        "        bpr_results[uid] = recommend_bpr_topk_for_user(user_to_index[uid], top_k=10)\n",
        "\n",
        "scores = []\n",
        "for uid, actual in test_truth.items():\n",
        "    preds = bpr_results.get(uid, [])\n",
        "    ap = average_precision_at_k(actual, preds, k=10)\n",
        "    scores.append(ap)\n",
        "\n",
        "bpr_map10 = np.mean(scores)\n",
        "print(\"MAP@10 BPR:\", bpr_map10)\n",
        "\n",
        "\n",
        "global_top10 = train_split['item_id'].value_counts().head(10).index.tolist()\n",
        "\n",
        "out_users = sorted(user_to_index.keys())\n",
        "\n",
        "rows = []\n",
        "for uid in out_users:\n",
        "    recs = bpr_results.get(uid, global_top10)\n",
        "    rows.append({'user_id': uid, 'item_id': \" \".join(map(str, recs))})\n",
        "\n",
        "sub_bpr = pd.DataFrame(rows)\n",
        "sub_bpr.to_csv(\"bpr_submission.csv\", index=False)\n",
        "\n",
        "print(\"Submission saved: bpr_submission.csv\")\n",
        "pd.read_csv(\"bpr_submission.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
